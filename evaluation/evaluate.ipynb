{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio as ta\n",
    "from data_utils import SpeechDataset, filter_yt_df, yt_data_to_df\n",
    "from eval_utils import _normalize_text, _wer\n",
    "from IPython.display import Audio\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sr = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = r'F:/BIG_FILES/AI_DATA/2024_STT'\n",
    "if not os.path.exists(video_dir):\n",
    "    print('Video directory not found')\n",
    "    raise FileNotFoundError(video_dir)\n",
    "    \n",
    "cache_dir = './cache'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_df, video_df, segment_df = yt_data_to_df(video_dir, do_load_transcripts=True)\n",
    "display(yt_df.head(3), video_df.head(3), segment_df.head(3))\n",
    "print(f'Number of videos: {len(video_df)}')\n",
    "print(f'Number of segments: {len(segment_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original number of segments: {len(yt_df)}\")\n",
    "print(f'Number of german segments: {len(yt_df[yt_df.language == \"de\"])}')\n",
    "print(f'Number of english segments: {len(yt_df[yt_df.language == \"en\"])}')\n",
    "print(f\"Number of auto generated segments: {len(yt_df[yt_df.is_generated == True])}\")\n",
    "print(f\"Number of manual segments: {len(yt_df[yt_df.is_generated == False])}\")\n",
    "print(f\"Total duration: {yt_df.segment_duration.sum() / 3600:.2f}h\")\n",
    "print(f\"Total duration (manual): {yt_df[yt_df.is_generated == False].segment_duration.sum() / 3600:.2f}h\")\n",
    "print(f\"Valid audio segments: {yt_df[yt_df.valid_audio].count().segment_id} / {len(yt_df)}\")\n",
    "min_segment_length = None\n",
    "max_segment_length = 30\n",
    "target_language = \"de\"\n",
    "use_auto_generated = False\n",
    "min_words = None\n",
    "max_words = None\n",
    "drop_columns = [\"language\", \"is_generated\", \"num_segments\", \"segment_durations\", \"segment_id\", \"valid_audio\"]\n",
    "yt_df_filtered = filter_yt_df(\n",
    "    yt_df,\n",
    "    min_segment_length=min_segment_length,\n",
    "    max_segment_length=max_segment_length,\n",
    "    language=target_language,\n",
    "    use_auto_generated=use_auto_generated,\n",
    "    min_words=min_words,\n",
    "    max_words=max_words,\n",
    ")\n",
    "yt_df_filtered = yt_df_filtered[yt_df_filtered.valid_audio].reset_index(drop=True)\n",
    "print(f\"Filtered number of segments: {len(yt_df_filtered)}\")\n",
    "print(f\"Total duration: {yt_df_filtered.segment_duration.sum() / 3600:.2f}h\")\n",
    "yt_df_filtered = yt_df_filtered.drop(columns=drop_columns)\n",
    "yt_df_filtered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of segment lengths\n",
    "yt_df_filtered.hist(column='segment_duration', bins=30, figsize=(10, 5), grid=False, color='#86bf91', zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of number of words\n",
    "yt_df_filtered.hist(column='num_words', bins=30, figsize=(10, 5), grid=False, color='#fe3e12', zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a random audio file\n",
    "sample_segment = yt_df_filtered.sample(1).iloc[0]\n",
    "audio_file_path = sample_segment['segment_path']\n",
    "transcript = sample_segment['transcript']\n",
    "wave, sr = ta.load(audio_file_path)\n",
    "print(f'Loaded audio file: {audio_file_path}')\n",
    "print(f'Wave shape: {wave.shape}')\n",
    "print(f'Sample rate: {sr}')\n",
    "display(Audio(wave.numpy(), rate=sr))\n",
    "\n",
    "res_wave = ta.transforms.Resample(sr, target_sr)(wave)\n",
    "print(f'Wave shape after resampling: {res_wave.shape}')\n",
    "display(Audio(res_wave.numpy(), rate=target_sr))\n",
    "\n",
    "# show the transcript but only x words per line\n",
    "words = transcript.split()\n",
    "words_per_line = 15\n",
    "for i in range(0, len(words), words_per_line):\n",
    "    print(' '.join(words[i:i+words_per_line]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_output(wave, sr, gt, decoded_output):\n",
    "    display(Audio(wave.numpy(), rate=sr))\n",
    "    print(f\"Ground Truth: {gt}\")\n",
    "    print(f\"Predicted:    {decoded_output.strip()}\")\n",
    "    print(f\"---\")\n",
    "    print(f\"Normalized Ground Truth: {_normalize_text(gt)}\")\n",
    "    print(f\"Normalized Predicted:    {_normalize_text(decoded_output)}\")\n",
    "    print(f'WER: {_wer(gt, decoded_output, _normalize_text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoProcessor, WhisperConfig,\n",
    "                          WhisperForConditionalGeneration)\n",
    "\n",
    "MODEL_ID = \"openai/whisper-small\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_config = WhisperConfig.from_pretrained(MODEL_ID, cache_dir=cache_dir)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID, config=whisper_config, cache_dir=cache_dir, torch_dtype=torch_dtype)\n",
    "whisper_model.eval().to('cuda')\n",
    "whisper_processor = AutoProcessor.from_pretrained(MODEL_ID, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_args = {\n",
    "    \"return_tensors\": \"pt\",\n",
    "    \"sampling_rate\": target_sr,\n",
    "}\n",
    "\n",
    "whisper_dataset = SpeechDataset(yt_df_filtered, whisper_processor, processor_args, target_sr)\n",
    "whisper_sample = whisper_dataset[0]\n",
    "whisper_loader = torch.utils.data.DataLoader(whisper_dataset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whisper_inferece(model, batch, processor):\n",
    "    input_features = [b[\"input_features\"] for b in batch]\n",
    "    input_features = torch.stack(input_features).squeeze(1).to('cuda').to(torch_dtype)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_features, language=\"de\")\n",
    "        decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n",
    "        del output, input_features\n",
    "        return decoded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_results = defaultdict(list)\n",
    "for batch in tqdm(whisper_loader):\n",
    "    decoded_outputs = whisper_inferece(whisper_model, batch, whisper_processor)\n",
    "    for i, decoded_output in enumerate(decoded_outputs):\n",
    "        whisper_results['decoded_output'].append(decoded_output.strip())\n",
    "        whisper_results['normalized_decoded_output'].append(_normalize_text(decoded_output))\n",
    "        whisper_results['gt'].append(batch[i]['transcript'].strip())\n",
    "        whisper_results['normalized_gt'].append(_normalize_text(batch[i]['transcript']))\n",
    "        whisper_results['audio_path'].append(batch[i]['audio_path'])\n",
    "    del decoded_outputs\n",
    "whisper_results_df = pd.DataFrame(whisper_results)\n",
    "# filter all rows where the gt is empty\n",
    "whisper_results_df['valid'] = whisper_results_df['normalized_gt'].apply(lambda x: len(x) > 0)\n",
    "whisper_results_df = whisper_results_df[whisper_results_df['valid']].reset_index(drop=True)\n",
    "whisper_results_df['wer'] = whisper_results_df.apply(lambda x: _wer(x['gt'], x['decoded_output'], _normalize_text), axis=1)\n",
    "whisper_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results and yt_df_filtered\n",
    "whisper_results_df.to_csv('whisper-small_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_wer = whisper_results_df['wer'].mean()*100\n",
    "print(f'WHISPER WER: {whisper_wer:.2f}%')\n",
    "\n",
    "# whisper_results_df[\"wer\"].describe()\n",
    "# print mean wer, median wer, std wer, min wer, max wer, 25%, 50%, 75% percentile\n",
    "print(\n",
    "    f\"{MODEL_ID}: WER: {whisper_results_df['wer'].mean():.2f}, Median: {whisper_results_df['wer'].median():.2f}, Std: {whisper_results_df['wer'].std():.2f}, Min: {whisper_results_df['wer'].min():.2f}, Max: {whisper_results_df['wer'].max():.2f}, 25%: {whisper_results_df['wer'].quantile(0.25):.2f}, 50%: {whisper_results_df['wer'].quantile(0.50):.2f}, 75%: {whisper_results_df['wer'].quantile(0.75):.2f}\"\n",
    ")\n",
    "\n",
    "# show example with the highest WER\n",
    "worst_wer_idx = whisper_results_df['wer'].idxmax()\n",
    "worst_wer_row = whisper_results_df.loc[worst_wer_idx]\n",
    "worst_wer_wave, sr = ta.load(worst_wer_row['audio_path'])\n",
    "show_output(worst_wer_wave, sr, worst_wer_row['gt'], worst_wer_row['decoded_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper-Large-Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoProcessor, WhisperConfig,\n",
    "                          WhisperForConditionalGeneration)\n",
    "\n",
    "MODEL_ID = \"primeline/distil-whisper-large-v3-german\"\n",
    "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "whisper_dist_config = WhisperConfig.from_pretrained(MODEL_ID, cache_dir=cache_dir)\n",
    "whisper_dist_model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID, config=whisper_dist_config, cache_dir=cache_dir, torch_dtype=torch_dtype)\n",
    "whisper_dist_model.eval().to('cuda')\n",
    "whisper_dist_processor = AutoProcessor.from_pretrained(MODEL_ID, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_args = {\n",
    "    \"return_tensors\": \"pt\",\n",
    "    \"sampling_rate\": target_sr,\n",
    "}\n",
    "\n",
    "whisper_dist_dataset = SpeechDataset(yt_df_filtered, whisper_dist_processor, processor_args, target_sr)\n",
    "whisper_dist_sample = whisper_dist_dataset[0]\n",
    "whisper_dist_loader = torch.utils.data.DataLoader(whisper_dist_dataset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whisper_dist_inferece(model, batch, processor):\n",
    "    input_features = [b[\"input_features\"] for b in batch]\n",
    "    input_features = torch.stack(input_features).squeeze(1).to('cuda').to(torch_dtype)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_features, language=\"de\")\n",
    "        decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n",
    "        del output, input_features\n",
    "        return decoded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_dist_results = defaultdict(list)\n",
    "for batch in tqdm(whisper_dist_loader):\n",
    "    decoded_outputs = whisper_dist_inferece(whisper_dist_model, batch, whisper_dist_processor)\n",
    "    for i, decoded_output in enumerate(decoded_outputs):\n",
    "        whisper_dist_results['decoded_output'].append(decoded_output.strip())\n",
    "        whisper_dist_results['normalized_decoded_output'].append(_normalize_text(decoded_output))\n",
    "        whisper_dist_results['gt'].append(batch[i]['transcript'].strip())\n",
    "        whisper_dist_results['normalized_gt'].append(_normalize_text(batch[i]['transcript']))\n",
    "        whisper_dist_results['audio_path'].append(batch[i]['audio_path'])\n",
    "    del decoded_outputs\n",
    "    break\n",
    "whisper_dist_results_df = pd.DataFrame(whisper_dist_results)\n",
    "# filter all rows where the gt is empty\n",
    "whisper_dist_results_df['valid'] = whisper_dist_results_df['normalized_gt'].apply(lambda x: len(x) > 0)\n",
    "whisper_dist_results_df = whisper_dist_results_df[whisper_dist_results_df['valid']].reset_index(drop=True)\n",
    "whisper_dist_results_df['wer'] = whisper_dist_results_df.apply(lambda x: _wer(x['gt'], x['decoded_output'], _normalize_text), axis=1)\n",
    "whisper_dist_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_dist_results_df.to_csv('distil-large-v3_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_dist_wer = whisper_dist_results_df['wer'].mean()*100\n",
    "print(f'WHISPER WER: {whisper_dist_wer:.2f}%')\n",
    "\n",
    "# whisper_dist_results_df[\"wer\"].describe()\n",
    "# print mean wer, median wer, std wer, min wer, max wer, 25%, 50%, 75% percentile\n",
    "print(\n",
    "    f\"{MODEL_ID}: WER: {whisper_dist_results_df['wer'].mean():.2f}, Median: {whisper_dist_results_df['wer'].median():.2f}, Std: {whisper_dist_results_df['wer'].std():.2f}, Min: {whisper_dist_results_df['wer'].min():.2f}, Max: {whisper_dist_results_df['wer'].max():.2f}, 25%: {whisper_dist_results_df['wer'].quantile(0.25):.2f}, 50%: {whisper_dist_results_df['wer'].quantile(0.50):.2f}, 75%: {whisper_dist_results_df['wer'].quantile(0.75):.2f}\"\n",
    ")\n",
    "\n",
    "# show example with the highest WER\n",
    "worst_wer_idx = whisper_dist_results_df['wer'].idxmax()\n",
    "worst_wer_row = whisper_dist_results_df.loc[worst_wer_idx]\n",
    "worst_wer_wave, sr = ta.load(worst_wer_row['audio_path'])\n",
    "show_output(worst_wer_wave, sr, worst_wer_row['gt'], worst_wer_row['decoded_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample output\n",
    "sample_idx = 14\n",
    "sample_row = whisper_dist_results_df.loc[sample_idx]\n",
    "sample_wave, sr = ta.load(sample_row['audio_path'])\n",
    "show_output(sample_wave, sr, sample_row['gt'], sample_row['decoded_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wav2Vec2ForCTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "MODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-german\"\n",
    "\n",
    "wav2vec_processor = Wav2Vec2Processor.from_pretrained(MODEL_ID, cache_dir=cache_dir)\n",
    "wav2vec_model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID, cache_dir=cache_dir)\n",
    "_= wav2vec_model.eval().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_args = {\n",
    "    \"return_tensors\": \"pt\",\n",
    "    # \"padding\":True,\n",
    "    \"sampling_rate\": target_sr,\n",
    "}\n",
    "\n",
    "wav2vec_dataset = SpeechDataset(yt_df_filtered, wav2vec_processor, processor_args, target_sr)\n",
    "wav2vec_sample = wav2vec_dataset[0]\n",
    "wav2vec_loader = torch.utils.data.DataLoader(wav2vec_dataset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2vec_inferece(model, batch, processor):\n",
    "    input_features = [b[\"input_features\"] for b in batch]\n",
    "    \n",
    "    # pad the input features\n",
    "    max_input_length = max([len(x[0]) for x in input_features])\n",
    "    input_features = [torch.nn.functional.pad(x[0], (0, max_input_length - x[0].shape[-1])) for x in input_features]\n",
    "    input_features = torch.stack(input_features).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        output = model(input_features).logits\n",
    "        predicted_ids = torch.argmax(output, dim=-1)\n",
    "        decoded_outputs = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "        del output, input_features\n",
    "        return decoded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_results = defaultdict(list)\n",
    "for batch in tqdm(wav2vec_loader):\n",
    "    decoded_outputs = wav2vec_inferece(wav2vec_model, batch, wav2vec_processor)\n",
    "    for i, decoded_output in enumerate(decoded_outputs):\n",
    "        wav2vec_results['decoded_output'].append(decoded_output.strip())\n",
    "        wav2vec_results['normalized_decoded_output'].append(_normalize_text(decoded_output))\n",
    "        wav2vec_results['gt'].append(batch[i]['transcript'].strip())\n",
    "        wav2vec_results['normalized_gt'].append(_normalize_text(batch[i]['transcript']))\n",
    "        wav2vec_results['audio_path'].append(batch[i]['audio_path'])\n",
    "    del decoded_outputs\n",
    "wav2vec_results_df = pd.DataFrame(wav2vec_results)\n",
    "# filter all rows where the gt is empty\n",
    "wav2vec_results_df['valid'] = wav2vec_results_df['normalized_gt'].apply(lambda x: len(x) > 0)\n",
    "wav2vec_results_df = wav2vec_results_df[wav2vec_results_df['valid']].reset_index(drop=True)\n",
    "wav2vec_results_df['wer'] = wav2vec_results_df.apply(lambda x: _wer(x['gt'], x['decoded_output'], _normalize_text), axis=1)\n",
    "wav2vec_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_results_df.to_csv('wav2vec_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_wer = wav2vec_results_df['wer'].mean()*100\n",
    "print(f'WAV2VEC WER: {wav2vec_wer:.2f}%')\n",
    "\n",
    "# wav2vec_results_df[\"wer\"].describe()\n",
    "# print mean wer, median wer, std wer, min wer, max wer, 25%, 50%, 75% percentile\n",
    "print(\n",
    "    f\"{MODEL_ID}: WER: {wav2vec_results_df['wer'].mean():.2f}, Median: {wav2vec_results_df['wer'].median():.2f}, Std: {wav2vec_results_df['wer'].std():.2f}, Min: {wav2vec_results_df['wer'].min():.2f}, Max: {wav2vec_results_df['wer'].max():.2f}, 25%: {wav2vec_results_df['wer'].quantile(0.25):.2f}, 50%: {wav2vec_results_df['wer'].quantile(0.50):.2f}, 75%: {wav2vec_results_df['wer'].quantile(0.75):.2f}\"\n",
    ")\n",
    "\n",
    "# show example with the highest WER\n",
    "worst_wer_idx = wav2vec_results_df['wer'].idxmax()\n",
    "worst_wer_row = wav2vec_results_df.loc[worst_wer_idx]\n",
    "worst_wer_wave, sr = ta.load(worst_wer_row['audio_path'])\n",
    "show_output(worst_wer_wave, sr, worst_wer_row['gt'], worst_wer_row['decoded_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample output\n",
    "sample_idx = 5\n",
    "sample_row = wav2vec_results_df.loc[sample_idx]\n",
    "sample_wave, sr = ta.load(sample_row['audio_path'])\n",
    "show_output(sample_wave, sr, sample_row['gt'], sample_row['decoded_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SeamlessM4Tv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, SeamlessM4Tv2Model\n",
    "\n",
    "MODEL_ID = \"facebook/seamless-m4t-v2-large\"\n",
    "\n",
    "seamless_processor = AutoProcessor.from_pretrained(MODEL_ID, cache_dir=cache_dir)\n",
    "seamless_model = SeamlessM4Tv2Model.from_pretrained(MODEL_ID, cache_dir=cache_dir)\n",
    "_ = seamless_model.eval().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model for ASR\n",
    "inputs = seamless_processor(audios=res_wave[0], return_tensors=\"pt\", sampling_rate=target_sr)\n",
    "inputs = inputs.to(\"cuda\")\n",
    "model_output = seamless_model.generate(**inputs, tgt_lang=\"deu\", generate_speech=False)\n",
    "decoded_output = seamless_processor.batch_decode(model_output[0], skip_special_tokens=True)[0]\n",
    "show_output(res_wave, target_sr, transcript, decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
